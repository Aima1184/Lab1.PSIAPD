# Лабораторная работа №1

## Исполнители

Обучающиеся группы 449м: Салимова Аим Фанилевна, Евсеенкова Кристина Денисовна

## Цель работы

Разработать и реализовать нейронную сеть AlexNet с оптимизатором AdaSmoothDelta, используя библиотеки Numpy и Torch. Провести оценку качества полученных моделей на тестовом наборе данных и провести сравнительный анализ быстродействия реализаций с использованием Numpy.

## Оглавление

*   [1. Теоретическая база](#1-теоретическая-база)
    *   [1.1 Основы машинного обучения](#11-основы-машинного-обучения)
    *   [1.2 Основные концепции глубокого обучения](#12-основные-концепции-глубокого-обучения)
    *   [1.3 Типы нейронных сетей](#13-типы-нейронных-сетей)
    *   [1.4 Алгоритмы обучения](#14-алгоритмы-обучения)
*   [2. Описание разработанной системы](#2-описание-разработанной-системы)
    *   [2.1 AlexNet](#21-alexnet)
    *   [2.2 AdaSmoothDelta](#22-adasmoothdelta)
*   [3. Результаты работы и тестирования системы](#3-результаты-работы-и-тестирования-системы)
*   [4. Выводы по работе](#4-выводы-по-работе)
*   [Список использованных источников](#список-использованных-источников)

## 1. Теоретическая база

### 1.1 Основы машинного обучения

Глубокое обучение является подразделом машинного обучения, которое, в свою очередь, является частью искусственного интеллекта. Основные понятия машинного обучения, применимые и к глубокому обучению, включают:

*   Обучающая выборка: Набор данных, используемый для обучения модели. Каждая запись в обучающей выборке состоит из входных данных (признаков) и соответствующей метки (целевой переменной) или целевого значения.
*   Функция потерь: Функция, которая определяет разницу между предсказаниями модели и истинными значениями в обучающей выборке. Цель обучения - минимизировать функцию потерь.
*   Градиентный спуск: Итеративный алгоритм оптимизации, который используется для минимизации функции потерь. Он заключается в вычислении градиента функции потерь по отношению к параметрам модели и обновления параметров в направлении, противоположном градиенту.
*   Переобучение: Ситуация, когда модель слишком хорошо адаптируется к обучающей выборке и плохо обобщается с новыми данными. Это происходит, когда модель запоминает шум и специфические детали обучающей выборки вместо того, чтобы учиться общим закономерностям.
*   Регуляризация: Методы, используемые для предотвращения переобучения.
*   Валидационная выборка: Набор данных, используемый для настройки гиперпараметров модели и оценки ее производительности во время обучения.
*   Тестовая выборка: Набор данных, используемый для окончательной оценки производительности модели после завершения обучения и настройки гиперпараметров.

### 1.2 Основные концепции глубокого обучения

Глубокое обучение отличается от традиционного машинного обучения использованием многослойных нейронных сетей (Deep Neural Networks, DNN).

*   Нейрон: Базовый элемент нейронной сети. Он получает на вход несколько значений, умножает их на соответствующие веса, суммирует результаты, добавляет смещение (bias) и применяет функцию активации.
*   Слой: Набор нейронов, объединенных вместе. Существуют различные типы слоев, такие как полносвязные слои, сверточные слои, слои объединения, рекуррентные слои и другие.
*   Веса: Параметры, которые умножаются на входные значения нейрона. Веса определяют силу связи между нейронами.
*   Смещение: Параметр, который добавляется к сумме взвешенных входных значений нейрона.
*   Функция активации: Нелинейная функция, которая применяется к выходу нейрона. Функция активации вводит нелинейность в работу сети, что позволяет ей моделировать сложные зависимости в данных.
*   Прямое распространение: Процесс вычисления выхода сети для заданного входного значения. Данные последовательно проходят через слои сети, начиная с входного слоя и заканчивая выходным слоем.
*   Обратное распространение: Алгоритм, используемый для вычисления градиентов функции потерь по отношению к параметрам сети. Этот алгоритм основан на правиле цепочки дифференцирования.
*   Оптимизация: Процесс поиска оптимальных значений параметров сети, которые минимизируют функцию потерь. Используются различные алгоритмы оптимизации, такие как градиентный спуск, стохастический градиентный спуск (SGD), Adam, RMSProp и другие.

### 1.3 Типы нейронных сетей

Существуют различные типы нейронных сетей, разработанных для решения различных задач.

*   Полносвязные нейронные сети: Каждый нейрон в одном слое связан со всеми нейронами в следующем слое. Хорошо подходят для решения задач классификации и регрессии, когда входные данные имеют фиксированную размерность.
*   Сверточные нейронные сети (CNN): Специально разработаны для обработки данных, имеющих структуру решетки, таких как изображения. CNN используют сверточные слои для извлечения признаков из изображений.
*   Рекуррентные нейронные сети: Предназначены для обработки последовательностей данных, таких как текст и временные ряды. Они имеют обратную связь, что позволяет им сохранять информацию о предыдущих элементах последовательности.
*   Трансформеры: Архитектура нейронных сетей, основанная на механизме внимания. Трансформеры показали отличные результаты в задачах обработки естественного языка, таких как машинный перевод и классификация текста.
*   Автоэнкодеры: Нейронные сети, предназначенные для обучения сжатому представлению данных (кодирования). Автоэнкодеры используются для снижения размерности данных, удаления шума и генерации новых данных.

### 1.4 Алгоритмы обучения

Существуют различные алгоритмы обучения, используемые для оптимизации параметров нейронных сетей.

*   Градиентный спуск: Итеративный алгоритм, который обновляет параметры модели в направлении, противоположном градиенту функции потерь.
*   Стохастический градиентный спуск: Вариант градиентного спуска, который использует только одну или несколько случайных записей из обучающей выборки для вычисления градиента на каждой итерации. Он может быть быстрее, чем обычный градиентный спуск, особенно для больших обучающих выборок.
*   Adam: Еще один адаптивный алгоритм оптимизации. Adam является одним из наиболее популярных алгоритмов оптимизации для глубокого обучения.

## 2. Описание разработанной системы

### 2.1 AlexNet

![image](https://neurohive.io/wp-content/uploads/2018/10/AlexNet-1-1.png)

Рисунок 1 – Архитектура сети AlexNet

AlexNet содержит восемь слоев с весовыми коэффициентами. Первые пять из них сверточные, а остальные три — полносвязные. Выход основанных на градиентном спуске, и является эволюцией алгоритмов RMSProp и AdaDelta. Ключевая идея AdaSmoothDelta заключается в адаптивной настройке скорости обучения для каждого параметра сети, основываясь на истории градиентов и изменений параметров. Важно отметить, что AdaSmoothDelta не проблему нестабильности обучения, характерную для многих адаптивных оптимизаторов, особенно в начале обучения или при использовании больших скоростей обучения. Он делает это, используя скользящие средние как градиентов, так и изменений параметров, чтобы смягчить резкие колебания и обеспечить более плавное и устойчивое схождение.

## 3. Результаты работы и тестирования системы 

В результате выполнения лабораторной работы была реализована нейронная сеть AlexNet с оптимизатором AdaSmoothDelta. Архитектура сети была реализована с использованием библиотеки Torch. Обучение модели проводилось с использованием двух различных оптимизаторов — Adam и AdaSmoothDelta. Результаты обучения нейронной сети с этими оптимизаторами представлены на графиках, показывающих изменения потерь и точности на обучающей и тестовой выборках в процессе обучения.

![5458737407416462704](https://github.com/user-attachments/assets/04a98855-f209-4cba-b922-7d9f91b0414f)

<center>Рисунок 2 - Результат обучения нейронной сети с оптимизатором Adam</center>

![5458737407416462705](https://github.com/user-attachments/assets/65a7cc49-fda3-4923-990d-5caaacf1c05b)

<center>Рисунок 3 - Результат обучения нейронной сети с оптимизатором AdaSmoothDelta</center>



## Выводы по работе

   На основании полученных данных можно сделать вывод, что оптимизатор Adam показал значительно лучшие результаты по сравнению с AdaSmoothDelta. Уже на первой эпохе точность на тестовой выборке составила 13.57%, и с каждой последующей эпохой наблюдалось устойчивое улучшение показателей. К 10-й эпохе точность на обучающей выборке достигла 88.86%, а на тестовой — 43.48%. Это свидетельствует о хорошей способности Adam эффективно минимизировать функцию потерь и обучать модель.

   В то же время оптимизатор AdaSmoothDelta продемонстрировал крайне низкие показатели. На протяжении всех эпох точность на обучающей выборке оставалась на уровне 0.83%, а на тестовой — 0.85%, при этом значения функции потерь практически не менялись. Это указывает на то, что данный оптимизатор не смог обеспечить эффективное обучение сети в предложенных условиях.

   Таким образом, можно заключить, что для данной архитектуры и используемых параметров обучения оптимизатор Adam является более подходящим, обеспечивая стабильное и прогрессивное обучение модели.



## Список использованных источников

1.  Хайкин, С. Нейронные сети : полный курс / С. Хайкин. – 2-е изд., испр. – Москва : Вильямс, 2006. – 1104 с.
2.  Гудфеллоу, А. Глубокое обучение / А. Гудфеллоу, И. Бенджио, А. Курвилль ; пер. с англ. – Москва : ДМК Пресс, 2018. – 652 с.
3.  Рассел, С. Искусственный интеллект: современный подход / С. Рассел, П. Норвиг ; пер. с англ. – 2-е изд. – Москва : Вильямс, 2006. – 1408 с.
4.  Иванов, Д. А. Анализ и прогнозирование временных рядов с использованием нейронных сетей / Д. А. Иванов // Информационные технологии. – 2018. – № 4. – С. 25-29.
5.  Петров, В. В. Разработка экспертной системы для диагностики заболеваний / В. В. Петров, С. И. Сидоров // Искусственный интеллект и принятие решений. – 2020. – № 2. – С. 15-22.
6.  Сергеев, А. С. Применение генетических алгоритмов для оптимизации параметров нейронных сетей / А. С. Сергеев // Проблемы управления. – 2019. – № 5. – С. 30-35.

